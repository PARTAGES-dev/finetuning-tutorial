{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ca33cbf",
   "metadata": {
    "id": "0ca33cbf"
   },
   "source": [
    "# ðŸ¥ Affinage (*fine-tuning*) de modÃ¨les dÃ©codeurs (LLMs) PARTAGES pour la rÃ©ponse aux questions Ã  choix multiples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdb9c84",
   "metadata": {
    "id": "5cdb9c84"
   },
   "source": [
    "### RÃ©sumÃ© du processus d'affinage\n",
    "\n",
    "Une mÃ©thode courante pour entraÃ®ner efficacement de grands modÃ¨les pour des tÃ¢ches en aval consiste Ã  insÃ©rer des matrices entraÃ®nables plus petites qui sont une dÃ©composition de bas rang (*Low-Rank Decomposition*) de la matrice de poids delta Ã  apprendre pendant le fine-tuning.\n",
    "Cette famille de techniques s'appelle donc **LoRA** (Low-Rank Adaptation).\n",
    "Elles permettent de rÃ©duire considÃ©rablement le nombre de paramÃ¨tres entraÃ®nables (*trainable parameters*) et ainsi le coÃ»t de calcul par rapport au fine-tuning complet oÃ¹ tout l'ensemble des paramÃ¨tres sont modifiÃ©es pour adapter le modÃ¨le Ã  la tÃ¢che.\n",
    "\n",
    "Pour plus d'informations sur LoRA\n",
    "- [Article fondamental](https://arxiv.org/abs/2106.09685)\n",
    "- [Article d'analyse thÃ©orique](https://openreview.net/pdf?id=likXVjmh3E)\n",
    "- [Documentation HF](https://huggingface.co/docs/peft/task_guides/lora_based_methods)\n",
    "\n",
    "En pratique, ce notebook implÃ©mente ce processus d'adaptation en 6 Ã©tapes :\n",
    "1. Chargement et prÃ©paration des donnÃ©es - on utilise ici un jeu de donnÃ©es de QCM mÃ©dicales hÃ©bergÃ© dans l'espace Hugging Face de l'Ã©quipe de dÃ©veloppement de PARTAGES.\n",
    "2. DÃ©finition des mÃ©triques Ã  rapporter - une fonction adaptÃ©e au filtrage des rÃ©ponses d'un modÃ¨le gÃ©nÃ©ratif spÃ©cifique Ã  la tÃ¢che de rÃ©ponses aux QCM.\n",
    "3. Chargement du modÃ¨le de base et son tokeniseur.\n",
    "4. PrÃ©paration de l'apprentissage - il faut ensuite dÃ©finir les mÃ©taparamÃ¨tres de LoRA ainsi que celles de l'optimisation de ses poids - taux d'apprentissage, taille de lot etc.\n",
    "5. Lancement de l'apprentissage.\n",
    "6. Ã‰valuation du modÃ¨le entraÃ®nÃ© (infÃ©rence)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cd14d3",
   "metadata": {
    "id": "e4cd14d3"
   },
   "source": [
    "### Installation des libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899be9c5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "899be9c5",
    "outputId": "442e33c1-e3b5-4f4a-9c74-0843e71f1cee"
   },
   "outputs": [],
   "source": [
    "# PyTorch\n",
    "%pip install torch\n",
    "\n",
    "# libraries Hugging Face\n",
    "%pip install 'transformers[torch]' datasets peft trl accelerate\n",
    "\n",
    "# autre\n",
    "%pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dd0579",
   "metadata": {
    "id": "62dd0579"
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "##########    Ã€ Modifier    ##########\n",
    "######################################\n",
    "\n",
    "# Si le jeu de donnÃ©e, modÃ¨le ou tokeniseur n'est pas public sur huggingface il\n",
    "# faut utiliser un jeton de connexion Ã  crÃ©er ici : https://huggingface.co/settings/tokens\n",
    "\n",
    "# Pour l'instant les modÃ¨les PARTAGES sont privÃ©s et accessibles aux personnes munies d'une clÃ© API\n",
    "HF_TOKEN = \"\"\n",
    "\n",
    "######################################\n",
    "\n",
    "# Petit hack pour Ã©viter un avertissement spÃ©cifique Ã  Google Colab sur le\n",
    "# chargement du jeton huggingface\n",
    "try:\n",
    "    from huggingface_hub.utils import _auth\n",
    "    _auth._get_token_from_google_colab = lambda: None\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9057d5d",
   "metadata": {
    "id": "d9057d5d"
   },
   "outputs": [],
   "source": [
    "################################\n",
    "### modules utiles gÃ©nÃ©rales ###\n",
    "################################\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Union\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a5d74d",
   "metadata": {
    "id": "f4a5d74d"
   },
   "outputs": [],
   "source": [
    "HERE = os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b301ca5",
   "metadata": {
    "id": "1b301ca5"
   },
   "source": [
    "### Chargement et prÃ©paration des donnÃ©es\n",
    "\n",
    "Les jeux de donnÃ©es pour lesquels ce tutoriel est conÃ§u, `PARTAGES-dev/frenchmedmcqa-sft` et `PARTAGES-dev/mediqal-sft` comprennent les caractÃ©ristiques suivantes :\n",
    "- `doc_id` : identifiants unique pour chaque document\n",
    "- `prompt` : l'entrÃ©e pour le modÃ¨le, structurÃ© pour les *chat templates* des LLMs avec une liste de dictionnaires avec des attributs `content` et `role` ; ici nous avons\n",
    "    - Le *system prompt* (`\"role\": \"system\"`) qui dÃ©finit le contexte de la tÃ¢che - ce message et le mÃªme pour tous les exemples dans `frenchmedmcqa` avec une petite variation en `mediqal` pour prÃ©ciser la spÃ©cialitÃ© mÃ©dicale qui fait l'objet de la question.\n",
    "    - La question Ã  laquelle le modÃ¨le doit rÃ©pondre (`\"role\": \"user\"`), avec 4-5 options diffÃ©rentes.\n",
    "- `completion` : la sortie attendue ; dans ce contexte, uniquement des lettres dÃ©signant la ou les bonnes rÃ©ponses.\n",
    "- `word_count` : Le nombre de mots total qui sera passÃ© au tokeniseur pour chaque question (y compris le system prompt).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019ca947",
   "metadata": {
    "id": "019ca947"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04111b6",
   "metadata": {
    "id": "b04111b6"
   },
   "source": [
    "#### Choisir un jeu de donnÃ©es\n",
    "\n",
    "Nous avons prÃ©parÃ© deux jeux de donnÃ©es composÃ©s de QCMs et formattÃ©s pour cette tÃ¢che.\n",
    "\n",
    "`frenchmedmcqa-sft` a 2 171 questions dans le jeu de *train* et 312 dans le jeu d'Ã©valuation, et `mediqal-sft` comprend 15 880 questions de train et 4 027 d'Ã©valuation.\n",
    "\n",
    "Modifiez la variable `DATASET_ID` pour choisir celui que vous allez utiliser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a70594",
   "metadata": {
    "id": "b1a70594"
   },
   "outputs": [],
   "source": [
    "DATASET_ID = 'PARTAGES-dev/frenchmedmcqa-sft'\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a482ff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "60a482ff",
    "outputId": "3cfd6c57-df90-4fe1-fbf4-c53e3250305c"
   },
   "outputs": [],
   "source": [
    "ds = load_dataset(DATASET_ID, token=HF_TOKEN)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59dc385",
   "metadata": {
    "id": "c59dc385"
   },
   "outputs": [],
   "source": [
    "train_ds_init = ds[\"train\"]\n",
    "eval_ds = ds[\"validation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11c7f6c",
   "metadata": {
    "id": "c11c7f6c"
   },
   "source": [
    "##### Jeu de dÃ©veloppement\n",
    "\n",
    "Nous allons utiliser un sous-ensemble du jeu de train pour suivre des mÃ©triques intrinsÃ©ques pendant l'entraÃ®nement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8520f843",
   "metadata": {
    "id": "8520f843"
   },
   "outputs": [],
   "source": [
    "dev_frac = .05  # Ã€ modifier si souhaitÃ©\n",
    "dev_split_ds = train_ds_init.train_test_split(test_size=dev_frac, shuffle=True, seed=RANDOM_SEED)\n",
    "train_ds = dev_split_ds[\"train\"]\n",
    "dev_ds = dev_split_ds[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14245ae",
   "metadata": {
    "id": "a14245ae"
   },
   "source": [
    "Regardons un exemple d'un document de ce jeu de donnÃ©es...\n",
    "\n",
    "(vÃ©rifiez qu'il correspond au schÃ©ma dÃ©crit au dessus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e363992",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6e363992",
    "outputId": "b4ad5360-f7a7-4595-a55c-9d610203919a"
   },
   "outputs": [],
   "source": [
    "example_instance = train_ds.shuffle()[0]  #Â Ã©chantilloner un exemple au hasard\n",
    "\n",
    "for k, v in example_instance.items():  #Â c'est un dictionnaire\n",
    "    if isinstance(v, list):  # prompt\n",
    "        print(f\"\\n* {k} *\")\n",
    "        for turn_name, str_ in v[0].items():\n",
    "            print(f\"{turn_name} : {str_}\")\n",
    "        continue\n",
    "    print(f\"\\n* {k} * : {v}\", end=\"\\n\" * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97bf111",
   "metadata": {
    "id": "b97bf111"
   },
   "source": [
    "### DÃ©finition des mÃ©triques a rapporter\n",
    "\n",
    "Nous allons Ã©valuer les rÃ©ponses du modÃ¨le de deux points de vue :\n",
    "1. Comme des rÃ©ponses Ã  un examen, ou la rÃ©ponse est, de maniÃ¨re globale, soit correcte soit fausse - d'oÃ¹ la mÃ©trique `exact_match`.\n",
    "2. Comme des estimations pour un problÃ¨me de classification - vu que nous avons un ensemble fixe de rÃ©ponses valides possibles, nous pouvons considÃ©rer le problÃ¨me comme une tÃ¢che de classification de documents multi-catÃ©gories. Nous calculons donc les mÃ©triques suivantes aussi, basÃ©es sur les Ã©lÃ©ments individuels (lettres A, B, etc.) de la rÃ©ponse :\n",
    "    - `precision` : parmi les rÃ©ponses du modÃ¨le, quelle proportion Ã©tait correcte ?  \n",
    "    - `recall` : parmi les rÃ©ponses correctes, combien ont Ã©tÃ© saisies par le modÃ¨le ?\n",
    "    - `f1` : moyenne harmonique de `precision` et `recall`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a86VgQmJocR",
   "metadata": {
    "id": "9a86VgQmJocR"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizerFast\n",
    "from peft import PeftModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ff3acb",
   "metadata": {
    "id": "e9ff3acb"
   },
   "outputs": [],
   "source": [
    "def mcqa_eval(\n",
    "    model: Union[PreTrainedModel, PeftModelForCausalLM],\n",
    "    tokenizer: PreTrainedTokenizerFast,\n",
    "    dataset: Dataset,\n",
    "    batch_size: int,\n",
    "    max_new_tokens: int,\n",
    "    return_all_outputs: bool = False,  # pour dÃ©boguer\n",
    ") -> Dict[str, float]:\n",
    "    if not tokenizer.pad_token_id:\n",
    "        tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token})\n",
    "\n",
    "    # compilation regex qui nous permettra de filter les sorties\n",
    "    letters = dataset.map(\n",
    "        lambda x: {'letters': re.sub(r'[^A-Z]', '', x['completion'][0]['content'])},\n",
    "        desc=\"Extracting MCQ answer options from evaluation dataset\",\n",
    "    )['letters']\n",
    "    answer_options = ''.join(set(''.join(letters)))\n",
    "    mcq_answer_pattern = re.compile(fr'[,\\.\\s>][{answer_options}][,\\.\\s<]')\n",
    "\n",
    "    # par souci d'efficacitÃ©, nous trions les sÃ©quences de tokens en fonction de leur longueurs\n",
    "    # cela permet d'utiliser le moins de tokens de \"padding\" possible\n",
    "    if not \"seq_len\" in dataset.features:\n",
    "        dataset = dataset.map(\n",
    "            lambda x: {\"seq_len\": len(x[\"input_ids\"])},\n",
    "            desc=\"Counting tokens\",\n",
    "        ).sort(\"seq_len\", reverse=False)\n",
    "    batched_dataset = dataset.batch(batch_size)\n",
    "\n",
    "    if return_all_outputs:\n",
    "        generations = {}\n",
    "        num_batch_tokens = []\n",
    "    total_docs = len(dataset)\n",
    "    eval_counts = defaultdict(int)  # celui va stocker les Ã©lÃ©ments d'entrÃ©e pour les mÃ©triques\n",
    "    generation_input_keys = \"input_ids\", \"attention_mask\"\n",
    "    for batch_idx, batch in enumerate(tqdm(\n",
    "        batched_dataset, desc=\"GÃ©nÃ©ration et Ã‰valuation\",\n",
    "    )):\n",
    "        batch_max_length = max(batch[\"seq_len\"])  # longueur de sÃ©quence maximale dans ce lot\n",
    "        batch_model_input = {k: v for k, v in batch.items() if k in generation_input_keys}\n",
    "        batch_tensors = tokenizer.pad(  # homogÃ©niser les longueurs de sÃ©quence pour pouvoir passer une matrice au modÃ¨le\n",
    "            batch_model_input, \"longest\", \"left\", return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "        if return_all_outputs:\n",
    "            batch_dim = batch_tensors[\"input_ids\"].shape\n",
    "            num_batch_tokens.append(batch_dim[0] * batch_dim[1])\n",
    "        with torch.no_grad():\n",
    "            # tourner le modÃ¨le\n",
    "            outputs = model.generate(\n",
    "                **batch_tensors,\n",
    "                top_p=.9,\n",
    "                temperature=.1,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "        output_ids = outputs.detach().cpu().numpy()  # rÃ©cuperer les tokens pour le CPU\n",
    "        output_text_list = tokenizer.batch_decode(output_ids)  # transformer les tokens de sortie en mots\n",
    "        for i, (id_, str_, label) in enumerate(\n",
    "            zip(batch[\"doc_id\"], output_text_list, batch[\"completion\"])\n",
    "        ):\n",
    "            input_text = tokenizer.decode(batch_tensors[\"input_ids\"][i])\n",
    "            answer_split_idx = len(input_text)\n",
    "            answer = str_[answer_split_idx:]\n",
    "            answer_matches = re.findall(mcq_answer_pattern, answer)\n",
    "            answer_set = set(map(lambda s: re.sub('[^A-Z]', '', s), answer_matches))\n",
    "            target = label[0][\"content\"].strip(\"\\n\")  # vÃ©ritÃ© terrain - la rÃ©ponse correcte\n",
    "            y_set = set(target.split(\",\"))\n",
    "            if answer_set:\n",
    "                overlap = answer_set.intersection(y_set)\n",
    "                num_correct_responses = len(overlap)  # vraies positives\n",
    "                num_incorrect_responses = len(answer_set - overlap)  # fausses positives\n",
    "                num_missed_responses = len(y_set - overlap)  # fausses nÃ©gatives\n",
    "                exact_match = int(num_correct_responses == len(y_set))\n",
    "            else:\n",
    "                num_correct_responses = num_incorrect_responses = exact_match = 0\n",
    "                num_missed_responses = len(y_set)\n",
    "            eval_counts[\"num_correct_responses\"] += num_correct_responses\n",
    "            eval_counts[\"num_incorrect_responses\"] += num_incorrect_responses\n",
    "            eval_counts[\"num_missed_responses\"] += num_missed_responses\n",
    "            eval_counts[\"exact_match\"] += exact_match\n",
    "            if return_all_outputs:\n",
    "                generations[id_] = str_\n",
    "    metrics = {}\n",
    "    metrics[\"accuracy\"] = eval_counts[\"exact_match\"] / len(dataset)\n",
    "    precision_denom = eval_counts[\"num_correct_responses\"] + eval_counts[\"num_incorrect_responses\"]\n",
    "    precision = (eval_counts[\"num_correct_responses\"] / precision_denom) if precision_denom else 0.\n",
    "    metrics[\"precision\"] = precision\n",
    "    recall_denom = eval_counts[\"num_correct_responses\"] + eval_counts[\"num_missed_responses\"]\n",
    "    recall = (eval_counts[\"num_correct_responses\"] / recall_denom) if recall_denom else 0.\n",
    "    metrics[\"recall\"] = recall\n",
    "    f1_denom = precision + recall\n",
    "    metrics[\"f1\"] = (2 * precision * recall / f1_denom) if f1_denom else 0.\n",
    "    ret = {\"metrics\": metrics}\n",
    "    if return_all_outputs:\n",
    "        ret[\"generations\"] = generations\n",
    "        ret[\"eval_counts\"] = eval_counts\n",
    "        ret[\"num_batch_tokens\"] = num_batch_tokens\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b530db5",
   "metadata": {
    "id": "2b530db5"
   },
   "source": [
    "### Chargement du tokeniseur et du modÃ¨le de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e699cc",
   "metadata": {
    "id": "07e699cc"
   },
   "outputs": [],
   "source": [
    "# Ici on sÃ©lectionne un des modÃ¨les du projet PARTAGES\n",
    "# Ã€ modifier pour utiliser un autre modÃ¨le comme base d'affinage\n",
    "PARTAGES_MODEL_ID = \"PARTAGES-dev/Qwen3-4B-PDAPT-SLERP\"\n",
    "BASE_MODEL_NAME = \"Qwen/Qwen3-4B-Base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683ddfea",
   "metadata": {
    "id": "683ddfea"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc95a8d2",
   "metadata": {
    "id": "cc95a8d2"
   },
   "outputs": [],
   "source": [
    "# Vu que nous avons adaptÃ© ce modÃ¨le Ã  partir d'un modÃ¨le de fondation existant, le tokeniseur est\n",
    "# identique Ã  celui de ce modÃ¨le de base\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, padding_side='left')\n",
    "# tokenizer.add_special_tokens({'pad_token': ''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e099f747",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "bd78082c5191431abefb701a033c1999",
      "bb1f0175b82d4c7dbef6bc46f3128720",
      "8a083bf1b74a4cfca9ada21042307e39",
      "44d51891188a4c5782e0569c01d10163",
      "f6d55f39a00640d6b8d1ab23728cc018",
      "70563b9176b047d29be1090ccee785b1",
      "9a6c2973256944cba39582d7fe3d9b39",
      "d8b21fa8e0b34fc58087187c58886e77",
      "79991e74ff304a39918d4d4325513877",
      "e3fb29250cc7456bb316c8d4c60fef29",
      "cd040b5820024f95a43d9845aa0e7f6f"
     ]
    },
    "id": "e099f747",
    "outputId": "80ccc41f-673e-4920-a946-4c32c3140d0d"
   },
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(PARTAGES_MODEL_ID, token=HF_TOKEN)\n",
    "# base_model.resize_token_embeddings(len(tokenizer), mean_resizing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6245325",
   "metadata": {
    "id": "b6245325"
   },
   "source": [
    "### PrÃ©paration de l'entrainement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b12321",
   "metadata": {
    "id": "48b12321"
   },
   "source": [
    "1. D'abord on fait la config de finetuning (LoraConfig)\n",
    "2. Puis on initialise des adaptateurs LORA sur le modÃ¨le de base, ce qui nous donne une version specialisÃ©e (`PeftModelForCausalLM`) de l'architecture\n",
    "3. Ensuite nous prÃ©parons les paramÃ¨tres d'apprentissage plus gÃ©nÃ©rales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee690559",
   "metadata": {
    "id": "ee690559"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aae141",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "26aae141",
    "outputId": "3245b89b-1391-42b9-d6c6-2bce910a9240"
   },
   "outputs": [],
   "source": [
    "## Configuration LoRA\n",
    "# cf. https://huggingface.co/docs/peft/v0.18.0/en/package_reference/lora#peft.LoraConfig\n",
    "modules_to_save = ['lm_head', 'embed_tokens']\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=10,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules='all-linear',\n",
    "    modules_to_save=modules_to_save,\n",
    "    task_type='CAUSAL_LM',\n",
    "    use_dora=True,\n",
    "    init_lora_weights='pissa_niter_16',\n",
    "    ensure_weight_tying=True\n",
    ")\n",
    "\n",
    "## Fusion du modÃ¨le de base avec les adaptateurs\n",
    "peft_model = get_peft_model(base_model, peft_config)\n",
    "print(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b811899",
   "metadata": {
    "id": "5b811899"
   },
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "from transformers import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba8fbcb",
   "metadata": {
    "id": "0ba8fbcb"
   },
   "outputs": [],
   "source": [
    "# crÃ©ation de fichier pour dÃ©finir le processus de traitement de prompts par le modÃ¨le\n",
    "# plus d'infos ici : [link]\n",
    "# vous pouvez Ã©galement utiliser un autre fichier .jinja si vous prÃ©fÃ©rez, ou bien customiser celui-ci.\n",
    "if hasattr(tokenizer, 'chat_template'):\n",
    "    chat_template_path = Path(HERE) / 'chat-template-tmp.jinja'\n",
    "    with chat_template_path.open('w') as f:\n",
    "        f.write(tokenizer.chat_template)\n",
    "    chat_template_path = str(chat_template_path)\n",
    "\n",
    "# le template dÃ©fini ci-dessus va s'occuper du rajout de certains tokens spÃ©ciaux nÃ©cessaire pour le bon\n",
    "# traitement des instructions, donc ici on prÃ©cise des arguments Ã  passer au SFTTrainer qui indiquera que\n",
    "# ces Ã©tapes ne sont pas nÃ©cessaire au niveau du traitement du jeu de donnÃ©es\n",
    "dataset_kwargs = {'add_special_tokens': False, 'append_concat_token': False}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4458ec9f",
   "metadata": {
    "id": "4458ec9f"
   },
   "source": [
    "Les rÃ©sultats de l'entraÃ®nement seront stockÃ© Ã  l'endroit prÃ©cisÃ© par `OUTPUT_DIR`.\n",
    "\n",
    "L'API `Trainer` sauvegarde des points de contrÃ´le (*checkpoints*) pendant l'entraÃ®nement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c412158",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7c412158",
    "outputId": "6eb1f4e0-f684-43f8-865f-2fabaffff543"
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#########################Â ParamÃ©tres d'apprentissage #########################\n",
    "### Ã€ modifier selon vos environnements, expÃ©rimentation, cas d'usage etc. ###\n",
    "##############################################################################\n",
    "\n",
    "output_name = Path(BASE_MODEL_NAME).name + \"-\" + Path(DATASET_ID).name\n",
    "OUTPUT_DIR = Path(os.getenv(\"HOME\")) / output_name  # chemin vers le dossier qui stockera les rÃ©sultats et les sorties\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Dossier de sortie:\", OUTPUT_DIR)\n",
    "\n",
    "# paramÃ¨tres qui gÃ©rent la durÃ©e de l'entraÃ®nement et l'utilisation de mÃ©moire\n",
    "# Ã  modifier selon les ressources de calcul disponibles\n",
    "EPOCHS = 4\n",
    "BATCH_SIZE_TR = 8\n",
    "BATCH_SIZE_EV = 8\n",
    "\n",
    "# accumulation de gradients - permet d'ajuster la taille de lot effective (_effective batch size_)\n",
    "# i.e. ici la taille de lot effective est BATCH_SIZE_TR * GRAD_ACC * NB_GPU\n",
    "GRAD_ACC = 2\n",
    "\n",
    "# dÃ©finition des algorithmes Ã  utiliser pour mettre Ã  jour les poids du modÃ¨le (le cÅ“ur de l'apprentissage)\n",
    "OPTIMIZER = 'adamw_torch_fused'\n",
    "LR_VALUE = 2e-5\n",
    "LR_SCHEDULE = 'linear'\n",
    "\n",
    "# configuration de l'apprentissage\n",
    "# cf. https://huggingface.co/docs/trl/v0.29.0/en/sft_trainer#trl.SFTConfig\n",
    "training_args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR / 'checkpoints',\n",
    "    logging_dir=OUTPUT_DIR / 'logs',\n",
    "    ####\n",
    "    # num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE_TR,\n",
    "    per_device_eval_batch_size=BATCH_SIZE_EV,\n",
    "    torch_empty_cache_steps=4,\n",
    "    ####\n",
    "    gradient_accumulation_steps=GRAD_ACC,\n",
    "    ####\n",
    "    optim=OPTIMIZER,\n",
    "    learning_rate=LR_VALUE,\n",
    "    lr_scheduler_type=LR_SCHEDULE,\n",
    "    warmup_ratio=.15,\n",
    "    max_grad_norm=.3,\n",
    "    completion_only_loss=True,\n",
    "    ####\n",
    "    chat_template_path=chat_template_path,\n",
    "    dataset_kwargs=dataset_kwargs,\n",
    "    ####\n",
    "    eval_steps=.1,  # tourner l'Ã©valuation sur `eval_ds` 10 fois pendant l'apprentissage\n",
    "    logging_steps=.05,  #  faire le logging (progression gÃ©nÃ©rale de l'apprentissage) 20 fois\n",
    "    save_strategy=\"epoch\",  # sauvegarder un checkpoint aprÃ¨s chaque Ã©poque\n",
    "    ####\n",
    "    bf16=False,  # dÃ©commente cette ligne en cas de 'ValueError: Your setup doesn't support bf16/gpu.'\n",
    "    seed=RANDOM_SEED,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1530075f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1530075f",
    "outputId": "471e8cb8-19b9-4a90-fd58-723656cf703a"
   },
   "outputs": [],
   "source": [
    "logging.set_verbosity_info()\n",
    "\n",
    "####### LANCEMENT DE L'APPRENTISSAGE #######\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294b6e8c",
   "metadata": {
    "id": "294b6e8c"
   },
   "source": [
    "Une fois l'entraÃ®nement lancÃ©, vous pouvez observer l'accumulation des points de contrÃ´le dans le sous-dossier `checkpoints` du dossier de sortie.\n",
    "\n",
    "Ã€ noter que si vous voulez vous servir Ã©ventuellement d'un point de contrÃ´le du modÃ¨le, il faudrait suivre les Ã©tapes de fusion dÃ©crites ci-dessous, comme pour le modÃ¨le final."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8989e5",
   "metadata": {
    "id": "6c8989e5"
   },
   "source": [
    "#### Fusion des couches LoRA avec le modÃ¨le de base\n",
    "\n",
    "Maintenant que nous avons entraÃ®nÃ© des modules *adaptateurs* pour la rÃ©ponse aux QCM, la derniÃ¨re Ã©tape consiste en les rÃ©fusionner avec les poids fondamentaux du modÃ¨le.\n",
    "\n",
    "Cela nous permet de ne pas rajouter du temps de calcul en infÃ©rence, contrairement Ã  d'autres mÃ©thodes d'affinage par adaptateurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a852d7",
   "metadata": {
    "id": "c6a852d7"
   },
   "outputs": [],
   "source": [
    "merged_model = trainer.model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ff3e5e",
   "metadata": {
    "id": "63ff3e5e"
   },
   "outputs": [],
   "source": [
    "# sauvegarder le modÃ¨le\n",
    "merged_model.save_pretrained(\n",
    "    OUTPUT_DIR,\n",
    "    safe_serialization=True,\n",
    "    max_shard_size=\"1GB\",\n",
    "    save_embedding_layers=True\n",
    ")\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e20abe",
   "metadata": {
    "id": "66e20abe"
   },
   "source": [
    "### Ã‰valuation du modÃ¨le entrainÃ© (infÃ©rence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbf4ae8-3d19-4ba6-be04-5e04fce71ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ds = eval_ds.map(\n",
    "    lambda x: tokenizer.apply_chat_template(\n",
    "        x[\"prompt\"],\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False,\n",
    "        return_dict=True,\n",
    "        padding=False  # fait par la fonction `mcqa_eval`\n",
    "    ),\n",
    "    desc=\"Tokenisation du jeu d'Ã©valuation\",\n",
    "    remove_columns=[\"prompt\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46d7227",
   "metadata": {
    "id": "c46d7227"
   },
   "outputs": [],
   "source": [
    "eval_results = mcqa_eval(\n",
    "    model=merged_model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=eval_ds,\n",
    "    batch_size=8,\n",
    "    max_new_tokens=16\n",
    ")\n",
    "metrics_2print = \"\\n\".join(f\"{k.upper()} = {round(v * 100, 2)}\" for k, v in eval_results[\"metrics\"].items())\n",
    "print(metrics_2print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8592d8",
   "metadata": {
    "id": "be8592d8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f840e2",
   "metadata": {
    "id": "a2f840e2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1832c5a7",
   "metadata": {
    "id": "1832c5a7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "44d51891188a4c5782e0569c01d10163": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e3fb29250cc7456bb316c8d4c60fef29",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_cd040b5820024f95a43d9845aa0e7f6f",
      "value": "â€‡310/310â€‡[00:01&lt;00:00,â€‡300.10it/s,â€‡Materializingâ€‡param=model.norm.weight]"
     }
    },
    "70563b9176b047d29be1090ccee785b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "79991e74ff304a39918d4d4325513877": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8a083bf1b74a4cfca9ada21042307e39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d8b21fa8e0b34fc58087187c58886e77",
      "max": 310,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_79991e74ff304a39918d4d4325513877",
      "value": 310
     }
    },
    "9a6c2973256944cba39582d7fe3d9b39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bb1f0175b82d4c7dbef6bc46f3128720": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_70563b9176b047d29be1090ccee785b1",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_9a6c2973256944cba39582d7fe3d9b39",
      "value": "Loadingâ€‡weights:â€‡100%"
     }
    },
    "bd78082c5191431abefb701a033c1999": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bb1f0175b82d4c7dbef6bc46f3128720",
       "IPY_MODEL_8a083bf1b74a4cfca9ada21042307e39",
       "IPY_MODEL_44d51891188a4c5782e0569c01d10163"
      ],
      "layout": "IPY_MODEL_f6d55f39a00640d6b8d1ab23728cc018"
     }
    },
    "cd040b5820024f95a43d9845aa0e7f6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d8b21fa8e0b34fc58087187c58886e77": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3fb29250cc7456bb316c8d4c60fef29": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f6d55f39a00640d6b8d1ab23728cc018": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
